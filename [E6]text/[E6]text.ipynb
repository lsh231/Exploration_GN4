{"cells":[{"cell_type":"markdown","source":["# 인공지능 작사가"],"metadata":{"id":"2X3-cZdHQ5rJ"}},{"cell_type":"markdown","source":["### I 다음 am을 쓰면 반 이상은 맞더라"],"metadata":{"id":"oCmJ_Seesgzm"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":554,"status":"ok","timestamp":1652954628836,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"Riw92_v73NIn","outputId":"7391421e-de24-437b-c9f0-6e58a6ad4761"},"outputs":[{"output_type":"stream","name":"stdout","text":["Source 문장: <start> 나는 밥을 먹었다 \n","Target 문장:  나는 밥을 먹었다 <end>\n"]}],"source":["sentence = \" 나는 밥을 먹었다 \"\n","\n","source_sentence = \"<start>\" + sentence\n","target_sentence = sentence + \"<end>\"\n","\n","print(\"Source 문장:\", source_sentence)\n","print(\"Target 문장:\", target_sentence)"]},{"cell_type":"markdown","source":["## 데이터 다듬기"],"metadata":{"id":"gZIkDIOhsuoK"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3373,"status":"ok","timestamp":1652954632655,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"ODhgNATj3gSU","outputId":"9adba57e-f831-4aef-c697-c467fb9b3031"},"outputs":[{"output_type":"stream","name":"stdout","text":["['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"]}],"source":["import os, re \n","import numpy as np\n","import tensorflow as tf\n","\n","# 파일을 읽기모드로 열고\n","# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n","file_path =  '/content/drive/MyDrive/AIFFEL/EXP/[E6]text/shakespeare.txt'\n","with open(file_path, \"r\") as f:\n","    raw_corpus = f.read().splitlines()\n","\n","# 앞에서부터 10라인만 화면에 출력해 볼까요?\n","print(raw_corpus[:9])"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1652954632656,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"267vuy9n5PeZ","outputId":"830767a8-7e5f-4c3a-a83a-5738e6760286"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before we proceed any further, hear me speak.\n","Speak, speak.\n","You are all resolved rather to die than to famish?\n"]}],"source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n","    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n","\n","    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n","        \n","    print(sentence)"]},{"cell_type":"markdown","source":["### 특수 문자 제거"],"metadata":{"id":"dLSb44Z_tSIu"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1652954632658,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"e92dxZC_5z0g","outputId":"5bfce445-19b4-4ba9-ae92-85cf413c2d43"},"outputs":[{"output_type":"stream","name":"stdout","text":["<start> this is sample sentence . <end>\n"]}],"source":["# 입력된 문장을\n","#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","#     2. 특수문자 양쪽에 공백을 넣고\n","#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","#     5. 다시 양쪽 공백을 지웁니다\n","#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1\n","    \n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5\n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence\n","\n","# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n","print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":811,"status":"ok","timestamp":1652954633461,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"eRqm-DN18kEm","outputId":"972c713a-e1fe-48bf-a08a-4c519bcbc79f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> before we proceed any further , hear me speak . <end>',\n"," '<start> speak , speak . <end>',\n"," '<start> you are all resolved rather to die than to famish ? <end>',\n"," '<start> resolved . resolved . <end>',\n"," '<start> first , you know caius marcius is chief enemy to the people . <end>',\n"," '<start> we know t , we know t . <end>',\n"," '<start> is t a verdict ? <end>',\n"," '<start> no more talking on t let it be done away , away ! <end>',\n"," '<start> one word , good citizens . <end>',\n"," '<start> we are accounted poor citizens , the patricians good . <end>']"]},"metadata":{},"execution_count":5}],"source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","    \n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    if len(preprocessed_sentence.split())>15:\n","      continue\n","      \n","    corpus.append(preprocessed_sentence)\n","\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1232,"status":"ok","timestamp":1652954634690,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"lgyDnEGe81BZ","outputId":"0b2e4b33-45cf-4a41-e4e9-6fd7963d4cf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[   2  141   40 ...    0    0    0]\n"," [   2  110    4 ...    0    0    0]\n"," [   2   11   49 ...    3    0    0]\n"," ...\n"," [   2  147 4524 ...    0    0    0]\n"," [   2   34   71 ...    3    0    0]\n"," [   2  930   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f2e4f66d750>\n"]}],"source":["# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n","# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","def tokenize(corpus):\n","    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=7000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor = tokenizer.texts_to_sequences(corpus)   \n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1652954634690,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"HQMNjoSq9ChN","outputId":"94712b1a-552c-47ec-f0c1-46c76ab8f1a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[   2  141   40  919  140  604    4  124   24  110]\n"," [   2  110    4  110    5    3    0    0    0    0]\n"," [   2   11   49   43 1181  308    9  196   74    9]]\n"]}],"source":["print(tensor[:3, :10])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1652954634691,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"NJn5iv0N9Kco","outputId":"dc839a4e-9e1c-462d-b802-e8b84d40d84a"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : .\n","6 : the\n","7 : and\n","8 : i\n","9 : to\n","10 : of\n"]}],"source":["for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1652954634691,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"B_K2HHmB9M6o","outputId":"f62a7d8b-73db-4457-cb1b-69adc013a23d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[  2 141  40 919 140 604   4 124  24 110   5   3   0   0]\n","[141  40 919 140 604   4 124  24 110   5   3   0   0   0]\n"]}],"source":["# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input = tensor[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input = tensor[:, 1:]    \n","\n","print(src_input[0])\n","print(tgt_input[0])"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","enc_train, enc_val, dec_train, dec_val = train_test_split( src_input, src_input, test_size=0.2, random_state=2)"],"metadata":{"id":"NOEbThcfZZ0-","executionInfo":{"status":"ok","timestamp":1652954634691,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1219,"status":"ok","timestamp":1652954635905,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"bj3hZH7-9p4R","outputId":"51210ff0-a753-4ca5-9a11-dc40bcc1ae10"},"outputs":[{"output_type":"stream","name":"stdout","text":["92\n"]}],"source":["BUFFER_SIZE = len(src_input)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(src_input) // BATCH_SIZE\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE = tokenizer.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","# 데이터셋에 대해서는 아래 문서를 참고하세요\n","# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n","# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset\n","\n","print(steps_per_epoch)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Tg8Ya9Tk-NrT","executionInfo":{"status":"ok","timestamp":1652954635906,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}}},"outputs":[],"source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(units = 128, return_sequences=True)\n","        self.rnn_2 = tf.keras.layers.LSTM(units = 128, return_sequences=True)\n","        self.rnn_3 = tf.keras.layers.LSTM(units = 128, return_sequences=True)\n","        self.rnn_4 = tf.keras.layers.LSTM(units = 128 , return_sequences=True)\n","        self.drop  = tf.keras.layers.Dropout(0.5)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.rnn_3(out)\n","        out = self.rnn_4(out)\n","        out = self.drop(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 256\n","hidden_size = 1024\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2396,"status":"ok","timestamp":1652954638297,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"bdnLdLFoDpTK","outputId":"746e0339-90c3-45f7-ba15-245a0d62cf4a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n","array([[[ 3.56553710e-06, -3.66374024e-06,  3.33420940e-06, ...,\n","          3.91724507e-06, -9.38156177e-07, -7.31692978e-07],\n","        [ 3.98815064e-06, -6.37040330e-06,  1.61752014e-05, ...,\n","          9.90091394e-06, -8.30023782e-06, -2.67064343e-06],\n","        [-1.16395277e-05, -5.70868042e-06,  3.27879352e-05, ...,\n","          1.17832597e-05, -2.12120394e-05, -1.53780329e-05],\n","        ...,\n","        [-6.08520000e-04,  3.10900563e-04,  5.96927421e-05, ...,\n","         -1.29475971e-04, -1.30081215e-04, -1.73603432e-04],\n","        [-6.19922474e-04,  3.75330361e-04,  5.34186147e-05, ...,\n","         -1.14704431e-04, -1.19941440e-04, -1.56632115e-04],\n","        [-6.20294188e-04,  4.28811210e-04,  4.87280377e-05, ...,\n","         -9.28123627e-05, -1.00792269e-04, -1.48267573e-04]],\n","\n","       [[ 3.56553710e-06, -3.66374024e-06,  3.33420940e-06, ...,\n","          3.91724507e-06, -9.38156177e-07, -7.31692978e-07],\n","        [ 1.68586539e-05, -1.51113109e-05,  1.12172684e-05, ...,\n","          9.65161053e-06, -1.34747586e-06, -9.39563233e-06],\n","        [ 3.85093081e-05, -3.39334911e-05,  1.51791164e-05, ...,\n","          1.98410416e-05,  2.33754395e-06, -3.05708199e-05],\n","        ...,\n","        [ 5.12777042e-05, -8.90130686e-05, -4.74203523e-04, ...,\n","         -1.63108067e-04,  2.55457653e-05, -2.33110492e-04],\n","        [ 4.09335844e-05, -9.66154839e-05, -5.93041594e-04, ...,\n","         -1.62778946e-04,  2.05056713e-05, -2.81165674e-04],\n","        [ 3.71240531e-05, -1.06408224e-04, -7.16578332e-04, ...,\n","         -1.57089351e-04,  1.45227214e-05, -3.43307795e-04]],\n","\n","       [[ 3.56553710e-06, -3.66374024e-06,  3.33420940e-06, ...,\n","          3.91724507e-06, -9.38156177e-07, -7.31692978e-07],\n","        [ 1.31131246e-05, -8.35722676e-06,  1.01268934e-05, ...,\n","          1.29402724e-05, -2.83307804e-06, -7.36018683e-06],\n","        [ 2.26340999e-05, -1.80770676e-05,  1.78303471e-05, ...,\n","          2.78757288e-05, -6.50245101e-06, -2.34574909e-05],\n","        ...,\n","        [-1.70424159e-04, -5.11257094e-05,  1.77172857e-04, ...,\n","         -9.67525411e-05,  1.26436507e-05, -9.44310668e-05],\n","        [-2.20452523e-04, -8.19975539e-05,  1.70489104e-04, ...,\n","         -9.92713103e-05,  2.98583909e-05, -1.26274797e-04],\n","        [-2.66905903e-04, -1.13294518e-04,  1.46856182e-04, ...,\n","         -9.10001982e-05,  4.69657534e-05, -1.54023714e-04]],\n","\n","       ...,\n","\n","       [[ 3.56553710e-06, -3.66374024e-06,  3.33420940e-06, ...,\n","          3.91724507e-06, -9.38156177e-07, -7.31692978e-07],\n","        [ 7.35000322e-06, -7.01396220e-06,  1.12590578e-05, ...,\n","          1.25419210e-05, -5.46773026e-06,  3.34363040e-06],\n","        [ 8.77490766e-06, -9.73436909e-06,  1.34168477e-05, ...,\n","          2.83260506e-05, -7.81628387e-06,  1.37627385e-05],\n","        ...,\n","        [-4.55115223e-04,  3.93624759e-05, -3.85156221e-04, ...,\n","          1.95304106e-04, -1.14057921e-05, -4.94261447e-04],\n","        [-5.16214583e-04,  3.66166496e-05, -3.99078417e-04, ...,\n","          2.10047452e-04, -3.06935362e-05, -5.79499581e-04],\n","        [-5.77377737e-04,  3.12214324e-05, -4.08522319e-04, ...,\n","          2.25746364e-04, -4.72953361e-05, -6.50617760e-04]],\n","\n","       [[ 3.56553710e-06, -3.66374024e-06,  3.33420940e-06, ...,\n","          3.91724507e-06, -9.38156177e-07, -7.31692978e-07],\n","        [ 4.86411227e-06, -7.38421704e-06,  2.58383557e-06, ...,\n","          9.06122114e-06,  1.82825704e-06, -5.28978808e-06],\n","        [-6.67330823e-07, -1.32525374e-05, -6.64470281e-06, ...,\n","          1.12272655e-05,  5.93528148e-06, -1.72088421e-05],\n","        ...,\n","        [-6.31641087e-05, -2.11722712e-04, -1.37305979e-04, ...,\n","          1.04801315e-04, -3.68538931e-05, -1.89325714e-04],\n","        [-4.34460635e-05, -2.23904382e-04, -1.41324286e-04, ...,\n","          1.08039530e-04, -4.42869605e-05, -1.87457394e-04],\n","        [-1.32894911e-05, -2.29992715e-04, -1.58828407e-04, ...,\n","          1.06991414e-04, -4.15471586e-05, -1.78675909e-04]],\n","\n","       [[ 3.56553710e-06, -3.66374024e-06,  3.33420940e-06, ...,\n","          3.91724507e-06, -9.38156177e-07, -7.31692978e-07],\n","        [ 1.83092216e-05, -7.39199095e-06,  7.84050098e-06, ...,\n","          7.89280875e-06, -1.30177554e-07, -5.27151224e-06],\n","        [ 3.81715836e-05, -3.29831801e-06,  8.09628364e-06, ...,\n","         -4.25140570e-06,  3.51746417e-06, -1.89596904e-05],\n","        ...,\n","        [ 7.78790854e-05,  2.89161195e-04, -4.21378820e-04, ...,\n","         -2.27139026e-04,  6.63316168e-05, -4.47856262e-04],\n","        [ 9.28321169e-05,  2.77412211e-04, -5.42914262e-04, ...,\n","         -2.06388955e-04,  7.95713204e-05, -5.46652358e-04],\n","        [ 1.08689288e-04,  2.56368541e-04, -6.72587543e-04, ...,\n","         -1.78192407e-04,  8.53065576e-05, -6.58175792e-04]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":13}],"source":["# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n","# 지금은 동작 원리에 너무 빠져들지 마세요~\n","for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1652954638299,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"IZHAUwpPD0Tb","outputId":"5ec314db-1901-4d15-b03e-ff7bb4547cb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"text_generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  1792256   \n","                                                                 \n"," lstm (LSTM)                 multiple                  197120    \n","                                                                 \n"," lstm_1 (LSTM)               multiple                  131584    \n","                                                                 \n"," lstm_2 (LSTM)               multiple                  131584    \n","                                                                 \n"," lstm_3 (LSTM)               multiple                  131584    \n","                                                                 \n"," dropout (Dropout)           multiple                  0         \n","                                                                 \n"," dense (Dense)               multiple                  903129    \n","                                                                 \n","=================================================================\n","Total params: 3,287,257\n","Trainable params: 3,287,257\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55035,"status":"ok","timestamp":1652954693323,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"8uDXxsqAEAHX","outputId":"45ca7435-b32a-40c3-bee9-cf3a4cc0f350"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","92/92 [==============================] - 10s 20ms/step - loss: 5.5931\n","Epoch 2/30\n","92/92 [==============================] - 2s 19ms/step - loss: 4.7751\n","Epoch 3/30\n","92/92 [==============================] - 2s 19ms/step - loss: 4.7516\n","Epoch 4/30\n","92/92 [==============================] - 2s 19ms/step - loss: 4.6562\n","Epoch 5/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.5157\n","Epoch 6/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.4391\n","Epoch 7/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.4112\n","Epoch 8/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.3986\n","Epoch 9/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.2776\n","Epoch 10/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.1293\n","Epoch 11/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.0775\n","Epoch 12/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.0491\n","Epoch 13/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.0258\n","Epoch 14/30\n","92/92 [==============================] - 2s 16ms/step - loss: 4.0037\n","Epoch 15/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.9793\n","Epoch 16/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.9526\n","Epoch 17/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.9293\n","Epoch 18/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.9101\n","Epoch 19/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.8924\n","Epoch 20/30\n","92/92 [==============================] - 2s 17ms/step - loss: 3.8780\n","Epoch 21/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.8617\n","Epoch 22/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.8475\n","Epoch 23/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.8310\n","Epoch 24/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.8164\n","Epoch 25/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.8021\n","Epoch 26/30\n","92/92 [==============================] - 2s 17ms/step - loss: 3.7866\n","Epoch 27/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.7725\n","Epoch 28/30\n","92/92 [==============================] - 2s 16ms/step - loss: 3.7564\n","Epoch 29/30\n","92/92 [==============================] - 2s 17ms/step - loss: 3.7405\n","Epoch 30/30\n","92/92 [==============================] - 2s 17ms/step - loss: 3.7268\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f2dc84fae90>"]},"metadata":{},"execution_count":15}],"source":["# optimizer와 loss등은 차차 배웁니다\n","# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n","# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n","# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(dataset, epochs=30)"]},{"cell_type":"code","source":["results = model.evaluate(enc_val,  dec_val, verbose=2)\n","\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-GozLO5YlMN","executionInfo":{"status":"ok","timestamp":1652954695971,"user_tz":-540,"elapsed":2655,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"39bafd56-a0f6-4dd1-9b36-6170650a8ccb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["148/148 - 3s - loss: 6.0263 - 3s/epoch - 17ms/step\n","6.026282787322998\n"]}]},{"cell_type":"code","execution_count":17,"metadata":{"id":"jznOmUtsEMOD","executionInfo":{"status":"ok","timestamp":1652954695973,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}}},"outputs":[],"source":["def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 단어 하나씩 예측해 문장을 만듭니다\n","    #    1. 입력받은 문장의 텐서를 입력합니다\n","    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n","    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n","    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n","    while True:\n","        # 1\n","        predict = model(test_tensor) \n","        # 2\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n","        # 3 \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        # 4\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1303,"status":"ok","timestamp":1652954697269,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"an50Ah15Hve7","outputId":"48ea1756-b166-4626-cb42-c7092564def4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> he , the <unk> , and the <unk> , <end> '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["generate_text(model, tokenizer, init_sentence=\"<start> he\")"]},{"cell_type":"markdown","source":["# 프로젝트 : 멋진 작사가 만들기"],"metadata":{"id":"OHBN--vWsLsv"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3291,"status":"ok","timestamp":1652960560042,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"dofBFYbIIliZ","outputId":"c1ef5be0-cb30-4c74-dca4-e610e5498a81"},"outputs":[{"output_type":"stream","name":"stdout","text":["glob2                         0.7\n","2.8.0\n"]}],"source":["import glob\n","import tensorflow\n","\n","!pip list | grep glob\n","print(tensorflow.__version__)"]},{"cell_type":"markdown","source":["데이터 읽기"],"metadata":{"id":"hCHPqSjRSRJ0"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18408,"status":"ok","timestamp":1652960601410,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"},"user_tz":-540},"id":"6Td876akIpUj","outputId":"690bc3ef-6bcb-4765-c290-b1a93b4febc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 크기: 187088\n","Examples:\n"," ['THE QUEEN _of_ HEARTS', '', '']\n"]}],"source":["import glob\n","import os\n","\n","txt_file_path = '/content/drive/MyDrive/AIFFEL/EXP/*text/data/data/lyrics/*'\n","\n","txt_list = glob.glob(txt_file_path)\n","# txt_list\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"]},{"cell_type":"markdown","source":["데이터 정제"],"metadata":{"id":"YsH4rX5nXfM9"}},{"cell_type":"code","source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n","    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n","\n","    if idx > 10: break   # 일단 문장 10개만 확인해 볼 겁니다.\n","        \n","    print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wskTPUcSd2A","executionInfo":{"status":"ok","timestamp":1652962225597,"user_tz":-540,"elapsed":365,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"98c3408c-9a43-4770-f12e-476eab65b724"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["THE QUEEN _of_ HEARTS\n","    The Queen of Hearts she made some tarts,\n","      All on a summer's day;\n","    The Knave of Hearts he stole those tarts,\n","      And took them clean away.\n","    The King of Hearts called for those tarts,\n","    And beat the Knave full sore.\n"]}]},{"cell_type":"code","source":["import re\n","\n","# 입력된 문장을\n","#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","#     2. 특수문자 양쪽에 공백을 넣고\n","#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","#     5. 다시 양쪽 공백을 지웁니다\n","#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5\n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence\n","\n","# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n","print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnkd9wc4SdyJ","executionInfo":{"status":"ok","timestamp":1652962233003,"user_tz":-540,"elapsed":343,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"3df4d0b9-3a5d-44c1-ff30-30b7e0f963a5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["<start> this is sample sentence . <end>\n"]}]},{"cell_type":"code","source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","    \n","    \n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AuNI34HMSdu4","executionInfo":{"status":"ok","timestamp":1652960620708,"user_tz":-540,"elapsed":2307,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"451e7a66-5bf7-439b-867f-f918bfebb5b9"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> the queen of hearts <end>',\n"," '<start> the queen of hearts she made some tarts , <end>',\n"," '<start> all on a summer s day <end>',\n"," '<start> the knave of hearts he stole those tarts , <end>',\n"," '<start> and took them clean away . <end>',\n"," '<start> the king of hearts called for those tarts , <end>',\n"," '<start> and beat the knave full sore . <end>',\n"," '<start> the knave of hearts brought back those tarts , <end>',\n"," '<start> and vowed he d steal no more . <end>',\n"," '<start> saint swithin s day <end>']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n","# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","def tokenize(corpus):\n","    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 15000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=20000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor = tokenizer.texts_to_sequences(corpus)   \n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 16)  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7s4T-bHiSdrZ","executionInfo":{"status":"ok","timestamp":1652962259311,"user_tz":-540,"elapsed":3984,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"749c657e-012e-4d2c-a73a-e26a06bad371"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  2   6 818 ...   0   0   0]\n"," [  2   6 818 ...   0   0   0]\n"," [  2  24  18 ...   0   0   0]\n"," ...\n"," [  2   5  90 ...   0   0   0]\n"," [  2   9 157 ...   0   0   0]\n"," [  2 160  15 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fec62252e50>\n"]}]},{"cell_type":"code","source":["for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xekxrd2Sdnh","executionInfo":{"status":"ok","timestamp":1652962264144,"user_tz":-540,"elapsed":346,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"ec8f3cca-c2b2-4f22-ad6c-d8a4067bf221"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : i\n","6 : the\n","7 : you\n","8 : and\n","9 : a\n","10 : to\n"]}]},{"cell_type":"markdown","source":["모델 만들기"],"metadata":{"id":"Gbsuidc8TRD_"}},{"cell_type":"code","source":["# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input = tensor[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input = tensor[:, 1:]    \n","\n","print(src_input[0])\n","print(tgt_input[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8mr8bpcSdPe","executionInfo":{"status":"ok","timestamp":1652962270798,"user_tz":-540,"elapsed":371,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"e4d80429-4433-4e00-db88-d04d533b2374"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[  2   6 818  19 953   3   0   0   0   0   0   0   0   0   0]\n","[  6 818  19 953   3   0   0   0   0   0   0   0   0   0   0]\n"]}]},{"cell_type":"code","source":["BUFFER_SIZE = len(src_input)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(src_input) // BATCH_SIZE\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE = tokenizer.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","# 데이터셋에 대해서는 아래 문서를 참고하세요\n","# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n","# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n","\n","dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset\n","\n","# dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n","# dataset = dataset.shuffle(BUFFER_SIZE)\n","# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","# dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0X9Lcw_XSfx","executionInfo":{"status":"ok","timestamp":1652960635899,"user_tz":-540,"elapsed":2867,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"4aad7eb3-c2c4-47e1-c8f0-c49d9bf2599e"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(256, 15), dtype=tf.int32, name=None), TensorSpec(shape=(256, 15), dtype=tf.int32, name=None))>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","enc_train, enc_val, dec_train, dec_val = train_test_split( src_input, src_input, test_size=0.2, random_state=2)"],"metadata":{"id":"aB838AxMjo97","executionInfo":{"status":"ok","timestamp":1652960637906,"user_tz":-540,"elapsed":2,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["train data"],"metadata":{"id":"YxUzyLSPUcIy"}},{"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNB6faEgjryM","executionInfo":{"status":"ok","timestamp":1652960653369,"user_tz":-540,"elapsed":358,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"65a1ee86-c08a-463e-e8a8-307f283af76e"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(256, 15), dtype=tf.int32, name=None), TensorSpec(shape=(256, 15), dtype=tf.int32, name=None))>"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["test data"],"metadata":{"id":"32W_ZMJnUfuy"}},{"cell_type":"code","source":["val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n","val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n","val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","val_dataset"],"metadata":{"id":"JKZLsi4kK9gI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652960655664,"user_tz":-540,"elapsed":366,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"71ad6839-c415-4e1e-bc56-3a09a41a7481"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(256, 15), dtype=tf.int32, name=None), TensorSpec(shape=(256, 15), dtype=tf.int32, name=None))>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# class TextGenerator(tf.keras.Model):\n","#     def __init__(self, vocab_size, embedding_size, hidden_size):\n","#         super().__init__()\n","        \n","#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","#         self.rnn_1 = tf.keras.layers.LSTM(units = 128, return_sequences=True)\n","#         self.rnn_2 = tf.keras.layers.LSTM(units = 64, return_sequences=True)\n","#         self.rnn_3 = tf.keras.layers.LSTM(units = 64, return_sequences=True)\n","#         self.rnn_4 = tf.keras.layers.LSTM(units = 64 , return_sequences=True)\n","#         self.rnn_5 = tf.keras.layers.LSTM(units = 32 , return_sequences=True)\n","#         self.drop  = tf.keras.layers.Dropout(0.5)\n","#         self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","#     def call(self, x):\n","#         out = self.embedding(x)\n","#         out = self.rnn_1(out)\n","#         out = self.rnn_2(out)\n","#         out = self.rnn_3(out)\n","#         out = self.rnn_4(out)\n","#         out = self.rnn_5(out)\n","#         out = self.drop(out)\n","#         out = self.linear(out)\n","        \n","#         return out\n","    \n","# embedding_size = 256\n","# hidden_size = 1024\n","# model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"],"metadata":{"id":"-ZD4D5GnXScc","executionInfo":{"status":"ok","timestamp":1652960658880,"user_tz":-540,"elapsed":2,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Model : Embedding layer - LSTM - LSTM - Dense"],"metadata":{"id":"Tt1yXFtVUiqj"}},{"cell_type":"code","source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 512\n","hidden_size = 1024\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"],"metadata":{"id":"4k72lacxVmod","executionInfo":{"status":"ok","timestamp":1652962962781,"user_tz":-540,"elapsed":347,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n","# 지금은 동작 원리에 너무 빠져들지 마세요~\n","for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RFsFtK_XSYu","executionInfo":{"status":"ok","timestamp":1652962966299,"user_tz":-540,"elapsed":1367,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"0156653b-05f5-419d-fcf4-d2a8812714f1"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 15, 20001), dtype=float32, numpy=\n","array([[[-3.17341146e-05,  3.20001447e-04,  2.13577478e-05, ...,\n","          6.90737652e-05, -2.26581440e-04, -3.52278003e-05],\n","        [-1.09710323e-04,  2.77915213e-04,  4.45113983e-05, ...,\n","          2.77722604e-04, -3.47339112e-04,  7.82162897e-05],\n","        [-3.11536918e-04,  4.21494799e-04,  9.55794094e-05, ...,\n","          3.68272740e-04, -4.49661195e-04,  1.99958216e-04],\n","        ...,\n","        [ 6.06095360e-04,  1.00569276e-03,  6.24806562e-04, ...,\n","         -4.55605448e-04, -1.37205279e-04,  1.70440541e-03],\n","        [ 7.72853615e-04,  6.48604357e-04,  7.29238091e-04, ...,\n","         -3.57809069e-04, -1.27376552e-04,  1.87512604e-03],\n","        [ 8.56984931e-04,  2.67052150e-04,  8.42046924e-04, ...,\n","         -2.08538157e-04, -1.10738350e-04,  2.00728001e-03]],\n","\n","       [[-3.17341146e-05,  3.20001447e-04,  2.13577478e-05, ...,\n","          6.90737652e-05, -2.26581440e-04, -3.52278003e-05],\n","        [-2.66347401e-04,  3.92277143e-04,  3.53512260e-05, ...,\n","         -2.76026731e-05, -4.17047937e-04,  2.73743819e-04],\n","        [-6.79354940e-04,  3.16547463e-04, -1.87224010e-04, ...,\n","          5.87710238e-05, -6.17631071e-04,  5.26886259e-04],\n","        ...,\n","        [ 1.46941748e-04, -1.51967665e-03,  1.44617166e-03, ...,\n","          4.80150891e-04, -2.23346087e-05,  1.91341690e-03],\n","        [ 1.28271553e-04, -1.63731619e-03,  1.58986810e-03, ...,\n","          6.67245826e-04,  9.76709707e-05,  2.02784082e-03],\n","        [ 1.16215801e-04, -1.72384083e-03,  1.70994864e-03, ...,\n","          8.34953098e-04,  2.18559551e-04,  2.14293948e-03]],\n","\n","       [[-3.17341146e-05,  3.20001447e-04,  2.13577478e-05, ...,\n","          6.90737652e-05, -2.26581440e-04, -3.52278003e-05],\n","        [ 1.22840982e-04,  5.37967775e-04,  2.22772622e-04, ...,\n","          2.98456944e-05, -6.86964893e-04,  7.62344325e-06],\n","        [ 2.60828238e-04,  6.50396862e-04,  1.05904350e-04, ...,\n","         -2.60159781e-04, -6.66149601e-04,  1.27739522e-05],\n","        ...,\n","        [-3.97626500e-05, -1.00950385e-03,  1.18741556e-03, ...,\n","         -6.22904219e-04,  1.00592338e-03, -1.46916855e-05],\n","        [-2.78318912e-04, -9.34215961e-04,  1.24359585e-03, ...,\n","         -9.37619712e-04,  1.27714674e-03, -1.47146435e-04],\n","        [-2.72194535e-04, -1.08721270e-03,  1.27403683e-03, ...,\n","         -1.17178145e-03,  1.27770670e-03, -9.19535305e-05]],\n","\n","       ...,\n","\n","       [[-3.17341146e-05,  3.20001447e-04,  2.13577478e-05, ...,\n","          6.90737652e-05, -2.26581440e-04, -3.52278003e-05],\n","        [ 1.01015845e-04,  8.48275260e-04, -1.42486184e-04, ...,\n","         -1.20676363e-04, -3.46385612e-04, -4.90044040e-05],\n","        [-3.99936180e-05,  9.77568561e-04, -2.90226104e-04, ...,\n","         -7.19748277e-05, -4.42809222e-04, -4.27184750e-05],\n","        ...,\n","        [-5.32841193e-04, -1.93075219e-04,  1.02003349e-03, ...,\n","          6.33543066e-04,  1.36432092e-04,  2.23346078e-03],\n","        [-4.42310935e-04,  1.72075641e-04,  1.16774091e-03, ...,\n","          3.44699074e-04,  3.43039021e-04,  2.31293309e-03],\n","        [-1.99105576e-04,  2.16606306e-04,  1.24487339e-03, ...,\n","          1.03961218e-04,  3.80757614e-04,  2.41975347e-03]],\n","\n","       [[-3.17341146e-05,  3.20001447e-04,  2.13577478e-05, ...,\n","          6.90737652e-05, -2.26581440e-04, -3.52278003e-05],\n","        [ 7.13449845e-05,  5.88388357e-04, -6.61196318e-05, ...,\n","          5.18808847e-05, -3.84018698e-04, -2.70404562e-04],\n","        [ 2.92730751e-04,  5.94227749e-04,  5.22823721e-05, ...,\n","          2.58931599e-04, -5.19602792e-04, -2.20490212e-04],\n","        ...,\n","        [-1.26725499e-04,  5.94548299e-04, -9.23890388e-04, ...,\n","         -6.40133920e-04, -6.60694437e-04,  1.70697967e-04],\n","        [ 9.88659522e-05,  8.49772769e-04, -7.61011033e-04, ...,\n","         -6.27092086e-04, -3.67350469e-04, -3.82851322e-05],\n","        [ 3.75812087e-04,  7.44969992e-04, -5.57749299e-04, ...,\n","         -6.06919115e-04, -2.29847879e-04,  5.31175974e-05]],\n","\n","       [[-3.17341146e-05,  3.20001447e-04,  2.13577478e-05, ...,\n","          6.90737652e-05, -2.26581440e-04, -3.52278003e-05],\n","        [-3.26277193e-04,  4.14493348e-04,  6.93234324e-05, ...,\n","          1.22360041e-04, -3.38271057e-04,  3.89521410e-05],\n","        [-4.53648739e-04,  5.99505729e-04,  5.21584996e-04, ...,\n","         -3.73596471e-04, -4.79812705e-04,  2.10668222e-04],\n","        ...,\n","        [ 4.67652775e-04, -9.39794176e-04,  1.82482344e-03, ...,\n","         -4.81841125e-04, -4.46686026e-04,  1.95921957e-03],\n","        [ 4.05953295e-04, -1.14765251e-03,  1.89534330e-03, ...,\n","         -1.75310852e-04, -2.97959748e-04,  2.03412841e-03],\n","        [ 3.40799132e-04, -1.31867186e-03,  1.95654994e-03, ...,\n","          1.16378127e-04, -1.42830278e-04,  2.11512693e-03]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGPgO0V7XSUa","executionInfo":{"status":"ok","timestamp":1652962967769,"user_tz":-540,"elapsed":6,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"3ccf6400-1595-41f2-89ad-f19c4c353294"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"text_generator_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     multiple                  10240512  \n","                                                                 \n"," lstm_2 (LSTM)               multiple                  6295552   \n","                                                                 \n"," lstm_3 (LSTM)               multiple                  8392704   \n","                                                                 \n"," dense_1 (Dense)             multiple                  20501025  \n","                                                                 \n","=================================================================\n","Total params: 45,429,793\n","Trainable params: 45,429,793\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# optimizer와 loss등은 차차 배웁니다\n","# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n","# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n","# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n","optimizer = tf.keras.optimizers.Adam()\n","#Loss\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","\n","model.fit(train_dataset, epochs=10, validation_data=val_dataset)"],"metadata":{"id":"3JjOMAwvEAq4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652964497054,"user_tz":-540,"elapsed":1527902,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"cdcba7bf-d80b-4bbd-98aa-737663be2934"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","549/549 [==============================] - 153s 276ms/step - loss: 2.3697 - val_loss: 1.0623\n","Epoch 2/10\n","549/549 [==============================] - 152s 277ms/step - loss: 0.7461 - val_loss: 0.5512\n","Epoch 3/10\n","549/549 [==============================] - 153s 278ms/step - loss: 0.4199 - val_loss: 0.3466\n","Epoch 4/10\n","549/549 [==============================] - 152s 277ms/step - loss: 0.2680 - val_loss: 0.2411\n","Epoch 5/10\n","549/549 [==============================] - 152s 278ms/step - loss: 0.1762 - val_loss: 0.1678\n","Epoch 6/10\n","549/549 [==============================] - 153s 278ms/step - loss: 0.1146 - val_loss: 0.1224\n","Epoch 7/10\n","549/549 [==============================] - 153s 278ms/step - loss: 0.0747 - val_loss: 0.0900\n","Epoch 8/10\n","549/549 [==============================] - 153s 278ms/step - loss: 0.0472 - val_loss: 0.0677\n","Epoch 9/10\n","549/549 [==============================] - 153s 278ms/step - loss: 0.0295 - val_loss: 0.0538\n","Epoch 10/10\n","549/549 [==============================] - 153s 278ms/step - loss: 0.0185 - val_loss: 0.0432\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fecdec0bf50>"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["results = model.evaluate(enc_val,  dec_val, verbose=2)\n","\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qeRlohNVoRnH","executionInfo":{"status":"ok","timestamp":1652964826589,"user_tz":-540,"elapsed":17532,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"485acbaf-cd5c-464a-f30d-9897fae4722f"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["1099/1099 - 17s - loss: 0.0431 - 17s/epoch - 16ms/step\n","0.04313502088189125\n"]}]},{"cell_type":"code","source":["def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 단어 하나씩 예측해 문장을 만듭니다\n","    #    1. 입력받은 문장의 텐서를 입력합니다\n","    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n","    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n","    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n","    while True:\n","        # 1\n","        predict = model(test_tensor) \n","        # 2\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n","        # 3 \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        # 4\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated"],"metadata":{"id":"3QwP_ApMoY7T","executionInfo":{"status":"ok","timestamp":1652964889103,"user_tz":-540,"elapsed":2,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"J4WU0qg8oY23","executionInfo":{"status":"ok","timestamp":1652964991680,"user_tz":-540,"elapsed":816,"user":{"displayName":"Sehan Lee","userId":"12512051884908843605"}},"outputId":"52c269e1-2a11-402b-ee1b-2bd449863c39"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> i love love love love love love love love love love love love love love love love love love '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":[""],"metadata":{"id":"f1UElw5XS-CN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.Train data와 evalution data로 분리하지 않고 source 데이타와 target데이터로 분리하였더니 매우 높은 평가 값이 나왔지만 evalution 데이터를 넣었더니 매우 낮은 값을 얻을 수 있었다.  \n","\n","2. 데이터 수정하는 과정에서 dataset을 tensor_slice, shuffle, 그리고 batch 하는 과정에서 동일한 이름의 dataset 이름을 적어 놓아야 하는데 그렇지 못하여 shape(256, 16)이어야 하는데 다른 dataset으로 하였는데 기존 데이터와 겹치면서 (256, 256, 16)으로 나와 model.fit이 실행이 안 되었다. 한 줄 한 줄 하나씩 보는 것이 중요한 것을 알 수 있었다."],"metadata":{"id":"dgCqBSj_U7OI"}}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"[E6]text.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1cpTQevvGWpCB91OIY5OkZW5QN5ie4fe-","authorship_tag":"ABX9TyMDLQDMNymoHvIwJGRxpBGW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}